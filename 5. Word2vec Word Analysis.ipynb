{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Term Analysis\n",
    "\n",
    "## aims\n",
    "\n",
    "* Augment LDA clustering with contextual understanding of key terms\n",
    "* Augment this contextual understanding with semantic algebra\n",
    "* Produce similar terms to network and interlink terms for anlaysis \n",
    "\n",
    "## method\n",
    "\n",
    "1. Load sanitised corpus, sentances & dictionary\n",
    "2. Train Word2Vec model \n",
    "3. Test for similar terms to core term\n",
    "4. Perform semantic algebra to further understand term context\n",
    "\n",
    "## questions\n",
    "\n",
    "* Of the most representitive LDA terms, how are they used?\n",
    "* Of core terms, how can we extract bias and perceptions?\n",
    "* How do these usages differ to our expectations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joemountford\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "# from gensim.models import LDA\n",
    "import gensim\n",
    "import logging\n",
    "import stop_words\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 11:33:05,957 : INFO : loading Dictionary object from Dictionary.dict\n",
      "2018-03-17 11:33:05,996 : INFO : loaded Dictionary.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82312\n",
      "59094\n",
      "2150151\n"
     ]
    }
   ],
   "source": [
    "#load corpus\n",
    "\n",
    "with open(\"corp.cor\", \"rb\") as cp: \n",
    "    corp = pickle.load(cp)\n",
    "with open(\"sentances2.sent\", \"rb\") as sent: \n",
    "    sentances = pickle.load(sent)\n",
    "dictionary = gensim.corpora.dictionary.Dictionary.load(\"Dictionary.dict\")\n",
    "\n",
    "print (len(corp))\n",
    "print (len(sentances))\n",
    "\n",
    "total_words = 0\n",
    "for snet in sentances: \n",
    "    for word in snet: \n",
    "        total_words += 1\n",
    "        \n",
    "print (total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model according to documentation defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 11:33:06,238 : INFO : collecting all words and their counts\n",
      "2018-03-17 11:33:06,239 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-17 11:33:06,325 : INFO : PROGRESS: at sentence #10000, processed 358880 words, keeping 28160 word types\n",
      "2018-03-17 11:33:06,409 : INFO : PROGRESS: at sentence #20000, processed 725730 words, keeping 41580 word types\n",
      "2018-03-17 11:33:06,503 : INFO : PROGRESS: at sentence #30000, processed 1090374 words, keeping 52865 word types\n",
      "2018-03-17 11:33:06,589 : INFO : PROGRESS: at sentence #40000, processed 1459146 words, keeping 62797 word types\n",
      "2018-03-17 11:33:06,681 : INFO : PROGRESS: at sentence #50000, processed 1819290 words, keeping 71638 word types\n",
      "2018-03-17 11:33:06,764 : INFO : collected 79140 word types from a corpus of 2150151 raw words and 59094 sentences\n",
      "2018-03-17 11:33:06,765 : INFO : Loading a fresh vocabulary\n",
      "2018-03-17 11:33:07,000 : INFO : min_count=2 retains 31661 unique words (40% of original 79140, drops 47479)\n",
      "2018-03-17 11:33:07,001 : INFO : min_count=2 leaves 2102672 word corpus (97% of original 2150151, drops 47479)\n",
      "2018-03-17 11:33:07,107 : INFO : deleting the raw counts dictionary of 79140 items\n",
      "2018-03-17 11:33:07,110 : INFO : sample=0.01 downsamples 0 most-common words\n",
      "2018-03-17 11:33:07,111 : INFO : downsampling leaves estimated 2102672 word corpus (100.0% of prior 2102672)\n",
      "2018-03-17 11:33:07,225 : INFO : estimated required memory for 31661 words and 300 dimensions: 91816900 bytes\n",
      "2018-03-17 11:33:07,226 : INFO : resetting layer weights\n",
      "2018-03-17 11:33:07,737 : INFO : training model with 4 workers on 31661 vocabulary and 300 features, using sg=0 hs=0 sample=0.01 negative=5 window=10\n",
      "2018-03-17 11:33:08,753 : INFO : EPOCH 1 - PROGRESS: at 26.55% examples, 550848 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-17 11:33:09,769 : INFO : EPOCH 1 - PROGRESS: at 52.81% examples, 548956 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:10,781 : INFO : EPOCH 1 - PROGRESS: at 80.56% examples, 558161 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-17 11:33:11,491 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-17 11:33:11,529 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-17 11:33:11,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-17 11:33:11,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-17 11:33:11,548 : INFO : EPOCH - 1 : training on 2150151 raw words (2102672 effective words) took 3.8s, 552692 effective words/s\n",
      "2018-03-17 11:33:12,578 : INFO : EPOCH 2 - PROGRESS: at 18.85% examples, 389786 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:13,579 : INFO : EPOCH 2 - PROGRESS: at 46.01% examples, 481959 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:14,604 : INFO : EPOCH 2 - PROGRESS: at 74.21% examples, 515330 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-17 11:33:15,477 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-17 11:33:15,517 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-17 11:33:15,524 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-17 11:33:15,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-17 11:33:15,528 : INFO : EPOCH - 2 : training on 2150151 raw words (2102672 effective words) took 4.0s, 532263 effective words/s\n",
      "2018-03-17 11:33:16,554 : INFO : EPOCH 3 - PROGRESS: at 19.29% examples, 392661 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:17,564 : INFO : EPOCH 3 - PROGRESS: at 46.01% examples, 475806 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-17 11:33:18,568 : INFO : EPOCH 3 - PROGRESS: at 74.70% examples, 517640 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:19,394 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-17 11:33:19,403 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-17 11:33:19,420 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-17 11:33:19,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-17 11:33:19,454 : INFO : EPOCH - 3 : training on 2150151 raw words (2102672 effective words) took 3.9s, 536729 effective words/s\n",
      "2018-03-17 11:33:20,478 : INFO : EPOCH 4 - PROGRESS: at 27.46% examples, 570805 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:21,484 : INFO : EPOCH 4 - PROGRESS: at 55.52% examples, 580765 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-17 11:33:22,495 : INFO : EPOCH 4 - PROGRESS: at 82.02% examples, 569936 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:23,105 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-17 11:33:23,123 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-17 11:33:23,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-17 11:33:23,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-17 11:33:23,163 : INFO : EPOCH - 4 : training on 2150151 raw words (2102672 effective words) took 3.7s, 569306 effective words/s\n",
      "2018-03-17 11:33:24,186 : INFO : EPOCH 5 - PROGRESS: at 26.07% examples, 542384 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:25,200 : INFO : EPOCH 5 - PROGRESS: at 52.81% examples, 549935 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:26,209 : INFO : EPOCH 5 - PROGRESS: at 80.56% examples, 559397 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-17 11:33:26,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-17 11:33:26,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-17 11:33:26,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-17 11:33:26,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-17 11:33:26,928 : INFO : EPOCH - 5 : training on 2150151 raw words (2102672 effective words) took 3.7s, 560841 effective words/s\n",
      "2018-03-17 11:33:26,929 : INFO : training on a 10750755 raw words (10513360 effective words) took 19.2s, 547831 effective words/s\n"
     ]
    }
   ],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 2  # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-2\n",
    "\n",
    "\n",
    "\n",
    "model = Word2Vec(sentances, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shit... that was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 11:33:26,938 : INFO : saving Word2Vec object under word2vec.vec, separately None\n",
      "2018-03-17 11:33:26,939 : INFO : not storing attribute vectors_norm\n",
      "2018-03-17 11:33:26,941 : INFO : not storing attribute cum_table\n",
      "2018-03-17 11:33:28,137 : INFO : saved word2vec.vec\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking term 'prince', what terms are used in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 11:33:30,269 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prince \n",
      "\n",
      "by word [('pocahontas', 0.7822363376617432), ('devout', 0.7541205286979675), ('persecutor', 0.7417248487472534), ('cinderella', 0.71034836769104), ('ephesian', 0.7089697122573853), ('dixie', 0.6935038566589355), ('fitzgerald', 0.6817329525947571), ('everlasting', 0.6791607737541199), ('mermaid', 0.6788723468780518), ('disney', 0.6758568286895752), ('sparrow', 0.6751775145530701), ('queen', 0.6721416711807251), ('penelope', 0.6675920486450195), ('tolstoy', 0.6667650938034058), ('plejune', 0.6611024737358093), ('virgin', 0.6601336598396301), ('europemaximus', 0.6543794870376587), ('strove', 0.6537072062492371), ('rebut', 0.6533612012863159), ('holy', 0.6514393091201782)]\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "term = \"prince\"\n",
    "\n",
    "term = lemma.lemmatize(term)\n",
    "\n",
    "print (term, \"\\n\")\n",
    "\n",
    "\n",
    "try:\n",
    "    x = model.wv.similar_by_word(term, topn = 20)\n",
    "    print (\"by word\", x)\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    print (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term 'princess' is used in LDA models, is it the equivalent of prince but with bias of promiscuity coming from the TRP concept of a 'bratty' type 'princess'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "princess \n",
      "\n",
      "by word [('limping', 0.6588572263717651), ('routed', 0.6575058102607727), ('emperor', 0.6438796520233154), ('brutally', 0.6360428929328918), ('spanish', 0.6316596269607544), ('o', 0.6258246898651123), ('shuts', 0.6160863637924194), ('babylon', 0.6156758069992065), ('merchant', 0.6121267080307007), ('melians', 0.6112450361251831), ('josemara', 0.60951167345047), ('tk', 0.6050773859024048), ('statesman', 0.5992505550384521), ('bean', 0.5910166501998901), ('philip', 0.5894211530685425), ('empire', 0.5837744474411011), ('fortress', 0.5818959474563599), ('wifebeater', 0.579513430595398), ('ruler', 0.5778967142105103), ('prefiguration', 0.576200544834137)]\n"
     ]
    }
   ],
   "source": [
    "term = \"princess\"\n",
    "\n",
    "term = lemma.lemmatize(term)\n",
    "\n",
    "print (term, \"\\n\")\n",
    "\n",
    "\n",
    "try:\n",
    "    x = model.wv.most_similar(positive= [term], negative = [\"slut\", \"whore\"], topn = 20)\n",
    "    print (\"by word\", x)\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure looks like it! \n",
    "\n",
    "Below is a rough attempt to build a dendrite diagram to demonstrate the clusters and interlinkage of similar terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joemountford\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#dendrite split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as scihy\n",
    "\n",
    "\n",
    "l = scihy.linkage(model.wv.syn0,\n",
    "            method= \"single\",\n",
    "            metric='seuclidean')\n",
    "\n",
    "# l = scihy.single(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('word')\n",
    "plt.xlabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    l,\n",
    "#     leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=16.,  # font size for the x axis labels\n",
    "    orientation='left',\n",
    "    leaf_label_func=lambda v: str(model.wv.index2word[v]),\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
